{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the full pipeline for the text anaylsis\n",
    "#There are two main components of analysis.\n",
    "    #1. Persuasion detection \n",
    "    #2. Analysis to classify arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd\n",
    "#import xgboost, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data from praw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acfuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acfuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acfuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Version 7.4.0 of praw is outdated. Version 7.5.0 was released 7 days ago.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carl_von_linne</td>\n",
       "      <td>I would like to wish a lovely weekend to every...</td>\n",
       "      <td>1637409587.0</td>\n",
       "      <td>hldkrgd</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monaarts</td>\n",
       "      <td>All the BTC owning Thanksgiving travelers must...</td>\n",
       "      <td>1637434076.0</td>\n",
       "      <td>hleyp7k</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CapSignificant1078</td>\n",
       "      <td>This is the final dip before the next dip.</td>\n",
       "      <td>1637424295.0</td>\n",
       "      <td>hlebi6n</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loskubster</td>\n",
       "      <td>Now that’s a candle</td>\n",
       "      <td>1637433630.0</td>\n",
       "      <td>hlexn61</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MrKittenz</td>\n",
       "      <td>How soon till will see the next: MicroStrategy...</td>\n",
       "      <td>1637387334.0</td>\n",
       "      <td>hlctj2b</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1021 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                                                  1  \\\n",
       "0         carl_von_linne  I would like to wish a lovely weekend to every...   \n",
       "1               monaarts  All the BTC owning Thanksgiving travelers must...   \n",
       "2     CapSignificant1078         This is the final dip before the next dip.   \n",
       "3             loskubster                                Now that’s a candle   \n",
       "4              MrKittenz  How soon till will see the next: MicroStrategy...   \n",
       "...                  ...                                                ...   \n",
       "1016                                                                          \n",
       "1017                                                                          \n",
       "1018                                                                          \n",
       "1019                                                                          \n",
       "1020                                                                          \n",
       "\n",
       "                 2        3      4          5          6   7  \n",
       "0     1637409587.0  hldkrgd  False  t3_qxyf7w  t3_qxyf7w  14  \n",
       "1     1637434076.0  hleyp7k  False  t3_qxyf7w  t3_qxyf7w  13  \n",
       "2     1637424295.0  hlebi6n  False  t3_qxyf7w  t3_qxyf7w  12  \n",
       "3     1637433630.0  hlexn61  False  t3_qxyf7w  t3_qxyf7w  10  \n",
       "4     1637387334.0  hlctj2b  False  t3_qxyf7w  t3_qxyf7w  11  \n",
       "...            ...      ...    ...        ...        ...  ..  \n",
       "1016                                                          \n",
       "1017                                                          \n",
       "1018                                                          \n",
       "1019                                                          \n",
       "1020                                                          \n",
       "\n",
       "[1021 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Praw\n",
    "from nltk.tag import pos_tag\n",
    "import text2emotion as te\n",
    "\n",
    "PULL_SIZE = 3\n",
    "subreddit = 'bitcoin'\n",
    "data = Praw.scrapeToCsv(PULL_SIZE, subreddit)\n",
    "#data = Praw.updateComments(data)\n",
    "pd.get_option(\"display.max_columns\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"comments.csv\")\n",
    "#data = Praw.updateComments(\"comments.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persuasion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Persuasion detection\n",
    "#In this part, we are using a classified machine learning algorithm. It is\n",
    "#Trained on ~70k reddit posts/comments that were gathered using PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('etc/persuasionExamples6.csv', encoding = \"latin1\", engine='python', usecols=['body', 'containsPersuasion'])\n",
    "data['containsPersuasion'] = np.where(data['containsPersuasion']=='[1]', 1, 0)\n",
    "data = data.astype('U')\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['body'] = data['body']\n",
    "trainDF['containsPersuasion'] = data['containsPersuasion']\n",
    "data['containsPersuasion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['body'], trainDF['containsPersuasion'])\n",
    "train_x = train_x.astype('U')\n",
    "valid_x = valid_x.astype('U')\n",
    "train_y = train_y.astype('U')\n",
    "valid_y = valid_y.astype('U')\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Count Vectorizer used in all 'XXCV' models.\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['body'])\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Word Vectorizer used in all 'XXWV' models.\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['body'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up N-gram Vectorizer used in all 'XXNV' models.\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['body'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def train_model(classifier, feature_vector_train, label):#, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    return classifier\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates model using above function, notice which training sets are passed for which model\n",
    "# Naive Bayes on Count Vectors\n",
    "nbcv = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y)\n",
    "predictions = nbcv.predict(xvalid_count)\n",
    "print(\"NBCV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "nbwv = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y)\n",
    "predictions = nbwv.predict(xvalid_tfidf)\n",
    "print(\"NBWV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "nbnv = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y)\n",
    "predictions = nbnv.predict(xvalid_tfidf_ngram)\n",
    "print(\"NBNV: \", metrics.accuracy_score(predictions, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "lrcv = train_model(linear_model.LogisticRegression(max_iter=1000000), xtrain_count, train_y)\n",
    "predictions = lrcv.predict(xvalid_count)\n",
    "print(\"LRCV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "lrwv = train_model(linear_model.LogisticRegression(max_iter=1000000), xtrain_tfidf, train_y)\n",
    "predictions = lrwv.predict(xvalid_tfidf)\n",
    "print(\"LRWV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "lrnv = train_model(linear_model.LogisticRegression(max_iter=1000000), xtrain_tfidf_ngram, train_y)\n",
    "predictions = lrnv.predict(xvalid_tfidf_ngram)\n",
    "print(\"LRNV: \", metrics.accuracy_score(predictions, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual testing, now that we have the classifiers trained, we can pass in our own tests.\n",
    "man_data = pd.read_csv('etc/testSet3.csv', encoding = \"latin1\", engine='python', usecols=['body', 'containsPersuasion'])\n",
    "man_x = man_data.body\n",
    "man_y = man_data.containsPersuasion\n",
    "#Have to use previous vectors.transform(man_x) to get right demensiosn.\n",
    "man_x_cv = count_vect.transform(man_x)\n",
    "man_x_wv = tfidf_vect.transform(man_x)\n",
    "man_x_nv = tfidf_vect_ngram.transform(man_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nbcv.predict(man_x_cv)\n",
    "print(\"NBCV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nbwv.predict(man_x_wv)\n",
    "print(\"NBWV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nbnv.predict(man_x_nv)\n",
    "print(\"NBNV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrcv.predict(man_x_cv)\n",
    "print(\"LRCV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrwv.predict(man_x_wv)\n",
    "print(\"LRWV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrnv.predict(man_x_nv)\n",
    "print(\"LRNV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing off confusion matrix of a specific algorithm\n",
    "#[is not persuasive and guessed right, is not persuasive but guessed wrong]\n",
    "#[Is persuasive but guessed wrong, is persuasicve and guessed right]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = nbnv.predict(man_x_nv)\n",
    "confusion_matrix = confusion_matrix(man_y, predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part two: Analysis to classify argument \n",
    "#In this part, we take all examples that were marked as persuasivem and do\n",
    "#Further analysis on them to estimate the classification of argument (Between \n",
    "#Logos, Ethos, and Pathos). Originally we wanted to do this via an unsupervised\n",
    "#algorithm, but have switched to a range of manual tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creats new dataframe containing only posts flagged as rhetoric\n",
    "data = temp[temp['containsPersuasion'] == '[-1]'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add If/Then/Num counts\n",
    "data['If_Count'] = data['Body'].apply(lambda x: x.lower().count(\"if\"))\n",
    "data['Then_Count'] = data['Body'].apply(lambda x:x.lower().count('then'))\n",
    "data['Num_Count'] = data['Body'].apply(lambda x: sum(c.isdigit() for c in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Counts for Proper Nouns\n",
    "data['Num_Ethos_Keys'] = data['Body'].apply(lambda x:len([word for word, pos, in pos_tag(x.split()) if pos == 'NNP' or pos == 'NNPS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Sentiment analysis on posts\n",
    "anger, fear, happy, sad, surprise = [],[],[],[],[]\n",
    "count = 0\n",
    "for _, row in data.iterrows():\n",
    "    if count % 100 ==0:\n",
    "        print(count)\n",
    "    count +=1\n",
    "    emotions = te.get_emotion(row['Body'])\n",
    "    anger.append(emotions.get(\"Angry\"))\n",
    "    fear.append(emotions.get(\"Fear\"))\n",
    "    happy.append(emotions.get(\"Happy\"))\n",
    "    sad.append(emotions.get(\"Sad\"))\n",
    "    surprise.append(emotions.get(\"Surprise\"))\n",
    "data[\"Anger_Score\"]=anger\n",
    "data[\"Fear_Score\"]=fear\n",
    "data[\"Happy_Score\"]=happy\n",
    "data[\"Sad_Score\"]=sad\n",
    "data[\"Surprise_Score\"]=surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Num_Logos_Keys\"] = data['If_Count']+data['Then_Count']+data['Num_Count']\n",
    "data.drop(columns=['If_Count', 'Then_Count', 'Num_Count'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
