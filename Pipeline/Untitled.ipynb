{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the full pipeline for the text anaylsis\n",
    "#There are two main components of analysis.\n",
    "    #1. Persuasion detection \n",
    "    #2. Analysis to classify arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import statements\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd\n",
    "#import xgboost, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data from praw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acfuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acfuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acfuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Version 7.4.0 of praw is outdated. Version 7.5.0 was released 7 days ago.\n",
      "C:\\Users\\acfuj\\Documents\\GitHub\\JuniorDesignML2\\Pipeline\\Praw.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dF[COMMENTS_CREATED] = dF[COMMENTS_CREATED].apply(lambda x: datetime.utcfromtimestamp(float(x)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>body</th>\n",
       "      <th>Created</th>\n",
       "      <th>Id</th>\n",
       "      <th>IsSubmitter</th>\n",
       "      <th>Link Id</th>\n",
       "      <th>Parent Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>containsPersuasion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carl_von_linne</td>\n",
       "      <td>I would like to wish a lovely weekend to every...</td>\n",
       "      <td>2021-11-20 11:59:47</td>\n",
       "      <td>hldkrgd</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monaarts</td>\n",
       "      <td>All the BTC owning Thanksgiving travelers must...</td>\n",
       "      <td>2021-11-20 18:47:56</td>\n",
       "      <td>hleyp7k</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CapSignificant1078</td>\n",
       "      <td>This is the final dip before the next dip.</td>\n",
       "      <td>2021-11-20 16:04:55</td>\n",
       "      <td>hlebi6n</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loskubster</td>\n",
       "      <td>Now thatâ€™s a candle</td>\n",
       "      <td>2021-11-20 18:40:30</td>\n",
       "      <td>hlexn61</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MrKittenz</td>\n",
       "      <td>How soon till will see the next: MicroStrategy...</td>\n",
       "      <td>2021-11-20 05:48:54</td>\n",
       "      <td>hlctj2b</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>t3_qxyf7w</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>Fotznbenutzernaml</td>\n",
       "      <td>Yes mate, I am aware that property exists whic...</td>\n",
       "      <td>2021-11-21 03:19:53</td>\n",
       "      <td>hlgsmpj</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qycrmc</td>\n",
       "      <td>t1_hlgej8j</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>php_questions</td>\n",
       "      <td>The same reason why you aren't borrowing money...</td>\n",
       "      <td>2021-11-21 02:31:07</td>\n",
       "      <td>hlgmzvl</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qycrmc</td>\n",
       "      <td>t1_hlgl6rc</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>Nzwiebach</td>\n",
       "      <td>ðŸ‘€ weâ€™re not borrowing money to buy bitcoin? I ...</td>\n",
       "      <td>2021-11-21 03:05:28</td>\n",
       "      <td>hlgqzih</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qycrmc</td>\n",
       "      <td>t1_hlgmzvl</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>Evil__Maid</td>\n",
       "      <td>Oh yea good point</td>\n",
       "      <td>2021-11-21 02:32:54</td>\n",
       "      <td>hlgn7d9</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qycrmc</td>\n",
       "      <td>t1_hlgmzvl</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>Evil__Maid</td>\n",
       "      <td>Reddit saved me from bankruptcy, again. Thanks :)</td>\n",
       "      <td>2021-11-21 03:15:23</td>\n",
       "      <td>hlgs4cr</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_qycrmc</td>\n",
       "      <td>t1_hlgqzih</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>893 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Author                                               body  \\\n",
       "0        carl_von_linne  I would like to wish a lovely weekend to every...   \n",
       "1              monaarts  All the BTC owning Thanksgiving travelers must...   \n",
       "2    CapSignificant1078         This is the final dip before the next dip.   \n",
       "3            loskubster                                Now thatâ€™s a candle   \n",
       "4             MrKittenz  How soon till will see the next: MicroStrategy...   \n",
       "..                  ...                                                ...   \n",
       "956   Fotznbenutzernaml  Yes mate, I am aware that property exists whic...   \n",
       "957       php_questions  The same reason why you aren't borrowing money...   \n",
       "958           Nzwiebach  ðŸ‘€ weâ€™re not borrowing money to buy bitcoin? I ...   \n",
       "959          Evil__Maid                                  Oh yea good point   \n",
       "960          Evil__Maid  Reddit saved me from bankruptcy, again. Thanks :)   \n",
       "\n",
       "                Created       Id IsSubmitter    Link Id   Parent Id Score  \\\n",
       "0   2021-11-20 11:59:47  hldkrgd       False  t3_qxyf7w   t3_qxyf7w    13   \n",
       "1   2021-11-20 18:47:56  hleyp7k       False  t3_qxyf7w   t3_qxyf7w    14   \n",
       "2   2021-11-20 16:04:55  hlebi6n       False  t3_qxyf7w   t3_qxyf7w    11   \n",
       "3   2021-11-20 18:40:30  hlexn61       False  t3_qxyf7w   t3_qxyf7w    10   \n",
       "4   2021-11-20 05:48:54  hlctj2b       False  t3_qxyf7w   t3_qxyf7w    10   \n",
       "..                  ...      ...         ...        ...         ...   ...   \n",
       "956 2021-11-21 03:19:53  hlgsmpj       False  t3_qycrmc  t1_hlgej8j     1   \n",
       "957 2021-11-21 02:31:07  hlgmzvl       False  t3_qycrmc  t1_hlgl6rc     4   \n",
       "958 2021-11-21 03:05:28  hlgqzih       False  t3_qycrmc  t1_hlgmzvl     1   \n",
       "959 2021-11-21 02:32:54  hlgn7d9       False  t3_qycrmc  t1_hlgmzvl     2   \n",
       "960 2021-11-21 03:15:23  hlgs4cr       False  t3_qycrmc  t1_hlgqzih     1   \n",
       "\n",
       "     containsPersuasion  \n",
       "0                    -1  \n",
       "1                    -1  \n",
       "2                    -1  \n",
       "3                    -1  \n",
       "4                    -1  \n",
       "..                  ...  \n",
       "956                  -1  \n",
       "957                  -1  \n",
       "958                  -1  \n",
       "959                  -1  \n",
       "960                  -1  \n",
       "\n",
       "[893 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Praw\n",
    "from nltk.tag import pos_tag\n",
    "import text2emotion as te\n",
    "\n",
    "PULL_SIZE = 3\n",
    "subreddit = 'bitcoin'\n",
    "data = Praw.scrapeToCsv(PULL_SIZE, subreddit)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"comments.csv\")\n",
    "#data = Praw.updateComments(\"comments.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persuasion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Persuasion detection\n",
    "#In this part, we are using a classified machine learning algorithm. It is\n",
    "#Trained on ~70k reddit posts/comments that were gathered using PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('etc/persuasionExamples6.csv', encoding = \"latin1\", engine='python', usecols=['body', 'containsPersuasion'])\n",
    "data['containsPersuasion'] = np.where(data['containsPersuasion']=='[1]', 1, 0)\n",
    "data = data.astype('U')\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['body'] = data['body']\n",
    "trainDF['containsPersuasion'] = data['containsPersuasion']\n",
    "data['containsPersuasion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['body'], trainDF['containsPersuasion'])\n",
    "train_x = train_x.astype('U')\n",
    "valid_x = valid_x.astype('U')\n",
    "train_y = train_y.astype('U')\n",
    "valid_y = valid_y.astype('U')\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Count Vectorizer used in all 'XXCV' models.\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['body'])\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Word Vectorizer used in all 'XXWV' models.\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['body'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up N-gram Vectorizer used in all 'XXNV' models.\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['body'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def train_model(classifier, feature_vector_train, label):#, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    return classifier\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates model using above function, notice which training sets are passed for which model\n",
    "# Naive Bayes on Count Vectors\n",
    "nbcv = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y)\n",
    "predictions = nbcv.predict(xvalid_count)\n",
    "print(\"NBCV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "nbwv = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y)\n",
    "predictions = nbwv.predict(xvalid_tfidf)\n",
    "print(\"NBWV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "nbnv = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y)\n",
    "predictions = nbnv.predict(xvalid_tfidf_ngram)\n",
    "print(\"NBNV: \", metrics.accuracy_score(predictions, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "lrcv = train_model(linear_model.LogisticRegression(max_iter=1000000), xtrain_count, train_y)\n",
    "predictions = lrcv.predict(xvalid_count)\n",
    "print(\"LRCV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "lrwv = train_model(linear_model.LogisticRegression(max_iter=1000000), xtrain_tfidf, train_y)\n",
    "predictions = lrwv.predict(xvalid_tfidf)\n",
    "print(\"LRWV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "lrnv = train_model(linear_model.LogisticRegression(max_iter=1000000), xtrain_tfidf_ngram, train_y)\n",
    "predictions = lrnv.predict(xvalid_tfidf_ngram)\n",
    "print(\"LRNV: \", metrics.accuracy_score(predictions, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0      I would like to wish a lovely weekend to every...\n",
      "1      All the BTC owning Thanksgiving travelers must...\n",
      "2             This is the final dip before the next dip.\n",
      "3                                    Now thatâ€™s a candle\n",
      "4      How soon till will see the next: MicroStrategy...\n",
      "                             ...                        \n",
      "956    Yes mate, I am aware that property exists whic...\n",
      "957    The same reason why you aren't borrowing money...\n",
      "958    ðŸ‘€ weâ€™re not borrowing money to buy bitcoin? I ...\n",
      "959                                    Oh yea good point\n",
      "960    Reddit saved me from bankruptcy, again. Thanks :)\n",
      "Name: body, Length: 893, dtype: object, 0     -1\n",
      "1     -1\n",
      "2     -1\n",
      "3     -1\n",
      "4     -1\n",
      "      ..\n",
      "956   -1\n",
      "957   -1\n",
      "958   -1\n",
      "959   -1\n",
      "960   -1\n",
      "Name: containsPersuasion, Length: 893, dtype: int64]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8ca5024e34a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mman_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"containsPersuasion\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mman_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mman_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mman_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mman_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mman_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainsPersuasion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#Have to use previous vectors.transform(man_x) to get right demensiosn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'body'"
     ]
    }
   ],
   "source": [
    "#Manual testing, now that we have the classifiers trained, we can pass in our own tests.\n",
    "#man_data = pd.read_csv('etc/testSet3.csv', encoding = \"latin1\", engine='python', usecols=['body', 'containsPersuasion'])\n",
    "manData = [data[\"body\"], data[\"containsPersuasion\"]]\n",
    "headers = [\"body\", \"containsPersuasion\"]\n",
    "man_data = pd.concat(manData, axis = 1)\n",
    "print(man_data)\n",
    "man_x = man_data.body\n",
    "man_y = man_data.containsPersuasion\n",
    "#Have to use previous vectors.transform(man_x) to get right demensiosn.\n",
    "man_x_cv = count_vect.transform(man_x)\n",
    "man_x_wv = tfidf_vect.transform(man_x)\n",
    "man_x_nv = tfidf_vect_ngram.transform(man_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nbcv.predict(man_x_cv)\n",
    "print(\"NBCV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nbwv.predict(man_x_wv)\n",
    "print(\"NBWV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nbnv.predict(man_x_nv)\n",
    "print(\"NBNV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrcv.predict(man_x_cv)\n",
    "print(\"LRCV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrwv.predict(man_x_wv)\n",
    "print(\"LRWV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrnv.predict(man_x_nv)\n",
    "print(\"LRNV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing off confusion matrix of a specific algorithm\n",
    "#[is not persuasive and guessed right, is not persuasive but guessed wrong]\n",
    "#[Is persuasive but guessed wrong, is persuasicve and guessed right]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = nbnv.predict(man_x_nv)\n",
    "confusion_matrix = confusion_matrix(man_y, predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part two: Analysis to classify argument \n",
    "#In this part, we take all examples that were marked as persuasivem and do\n",
    "#Further analysis on them to estimate the classification of argument (Between \n",
    "#Logos, Ethos, and Pathos). Originally we wanted to do this via an unsupervised\n",
    "#algorithm, but have switched to a range of manual tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creats new dataframe containing only posts flagged as rhetoric\n",
    "data = temp[temp['containsPersuasion'] == '[-1]'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add If/Then/Num counts\n",
    "data['If_Count'] = data['Body'].apply(lambda x: x.lower().count(\"if\"))\n",
    "data['Then_Count'] = data['Body'].apply(lambda x:x.lower().count('then'))\n",
    "data['Num_Count'] = data['Body'].apply(lambda x: sum(c.isdigit() for c in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Counts for Proper Nouns\n",
    "data['Num_Ethos_Keys'] = data['Body'].apply(lambda x:len([word for word, pos, in pos_tag(x.split()) if pos == 'NNP' or pos == 'NNPS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Sentiment analysis on posts\n",
    "anger, fear, happy, sad, surprise = [],[],[],[],[]\n",
    "count = 0\n",
    "for _, row in data.iterrows():\n",
    "    if count % 100 ==0:\n",
    "        print(count)\n",
    "    count +=1\n",
    "    emotions = te.get_emotion(row['Body'])\n",
    "    anger.append(emotions.get(\"Angry\"))\n",
    "    fear.append(emotions.get(\"Fear\"))\n",
    "    happy.append(emotions.get(\"Happy\"))\n",
    "    sad.append(emotions.get(\"Sad\"))\n",
    "    surprise.append(emotions.get(\"Surprise\"))\n",
    "data[\"Anger_Score\"]=anger\n",
    "data[\"Fear_Score\"]=fear\n",
    "data[\"Happy_Score\"]=happy\n",
    "data[\"Sad_Score\"]=sad\n",
    "data[\"Surprise_Score\"]=surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Num_Logos_Keys\"] = data['If_Count']+data['Then_Count']+data['Num_Count']\n",
    "data.drop(columns=['If_Count', 'Then_Count', 'Num_Count'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
