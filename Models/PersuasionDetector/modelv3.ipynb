{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f57f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###IMPORTING STUFF###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4239a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "####SETTING UP DATA SETS##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cc1f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    47303\n",
       "1    30890\n",
       "Name: containsPersuasion, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('persuasionExamples6.csv', encoding = \"latin1\", engine='python', usecols=['body', 'containsPersuasion'])\n",
    "data['containsPersuasion'] = np.where(data['containsPersuasion']=='[1]', 1, 0)\n",
    "data = data.astype('U')\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['body'] = data['body']\n",
    "trainDF['containsPersuasion'] = data['containsPersuasion']\n",
    "data['containsPersuasion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3627877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['body'], trainDF['containsPersuasion'])\n",
    "train_x = train_x.astype('U')\n",
    "valid_x = valid_x.astype('U')\n",
    "train_y = train_y.astype('U')\n",
    "valid_y = valid_y.astype('U')\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Count vectorizer used for all 'CV' models#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2387a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizer!!\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['body'])\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Word vectorizer used for all 'WV' models#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dc6d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['body'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3440f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####N-Gram vectorizer used for all 'NV' models#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc353028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['body'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b19703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Creates embedded words? I copied this form the website linked in teams###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8fa3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['body'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc808a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Following method takes in a classifier, and trains is against the given input/expected vectors#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77686774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def train_model(classifier, feature_vector_train, label):#, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    return classifier\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed983b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Creates model using above function, notice which training sets are passed for which model#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be54c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBCV:  0.7834160315105632\n",
      "NBWV:  0.8750319709448053\n",
      "NBNV:  0.8443910174433474\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "nbcv = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y)\n",
    "predictions = nbcv.predict(xvalid_count)\n",
    "print(\"NBCV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "nbwv = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y)\n",
    "predictions = nbwv.predict(xvalid_tfidf)\n",
    "print(\"NBWV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "nbnv = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y)\n",
    "predictions = nbnv.predict(xvalid_tfidf_ngram)\n",
    "print(\"NBNV: \", metrics.accuracy_score(predictions, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a506b6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRCV:  0.9185636093917847\n",
      "LRWV:  0.9133459511995499\n",
      "LRNV:  0.8675124047265845\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "lrcv = train_model(linear_model.LogisticRegression(max_iter=1000000), xtrain_count, train_y)\n",
    "predictions = lrcv.predict(xvalid_count)\n",
    "print(\"LRCV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "lrwv = train_model(linear_model.LogisticRegression(max_iter=1000000), xtrain_tfidf, train_y)\n",
    "predictions = lrwv.predict(xvalid_tfidf)\n",
    "print(\"LRWV: \", metrics.accuracy_score(predictions, valid_y))\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "lrnv = train_model(linear_model.LogisticRegression(max_iter=1000000), xtrain_tfidf_ngram, train_y)\n",
    "predictions = lrnv.predict(xvalid_tfidf_ngram)\n",
    "print(\"LRNV: \", metrics.accuracy_score(predictions, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76368457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MANUAL, now that we have the classifiers trained, we can pass in our own tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae808e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "man_data = pd.read_csv('testSet3.csv', encoding = \"latin1\", engine='python', usecols=['body', 'containsPersuasion'])\n",
    "man_x = man_data.body\n",
    "man_y = man_data.containsPersuasion\n",
    "#Have to use previous vectors.transform(man_x) to get right demensiosn.\n",
    "man_x_cv = count_vect.transform(man_x)\n",
    "man_x_wv = tfidf_vect.transform(man_x)\n",
    "man_x_nv = tfidf_vect_ngram.transform(man_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a725536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBCV:  0.75\n"
     ]
    }
   ],
   "source": [
    "predictions = nbcv.predict(man_x_cv)\n",
    "print(\"NBCV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "397eafd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBWV:  0.625\n"
     ]
    }
   ],
   "source": [
    "predictions = nbwv.predict(man_x_wv)\n",
    "print(\"NBWV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38d8a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBNV:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions = nbnv.predict(man_x_nv)\n",
    "print(\"NBNV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffc43e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRCV:  0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions = lrcv.predict(man_x_cv)\n",
    "print(\"LRCV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0d73ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRWV:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "predictions = lrwv.predict(man_x_wv)\n",
    "print(\"LRWV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11aa1870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRNV:  0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions = lrnv.predict(man_x_nv)\n",
    "print(\"LRNV: \", metrics.accuracy_score(predictions, man_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
