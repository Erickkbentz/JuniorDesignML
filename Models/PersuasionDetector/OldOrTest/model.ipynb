{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b84583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\erice\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\erice\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\erice\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\erice\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# VERY GOOD TUTORIAL: https://towardsdatascience.com/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267\n",
    "import nltk\n",
    "import csv\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b527013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stuff ans set rules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 2)\n",
    "pd.set_option('display.width', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c18080f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    49240\n",
       "0    47467\n",
       "Name: containsPersuasion, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read csv and binary encode the containsPersuasion col\n",
    "sample = pd.read_csv('persuasionExamples.csv', encoding = \"latin1\", engine='python', usecols=['body', 'containsPersuasion'])\n",
    "#sample = pd.DataFrame(temp, columns=['body', 'containsPersuasion'])\n",
    "sample.head()\n",
    "\n",
    "sample['containsPersuasion'] = np.where(sample['containsPersuasion']=='[1]', 1, 0)\n",
    "sample['containsPersuasion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55795274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dimensions: ((67694,), (67694,))\n",
      "Test dimensions: ((29013,), (29013,))\n",
      "1    34360\n",
      "0    33334\n",
      "Name: containsPersuasion, dtype: int64\n",
      "1    14880\n",
      "0    14133\n",
      "Name: containsPersuasion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#break data into a training and validation set, .70 of the data is to train, .30 is to validate\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample['body'], sample['containsPersuasion'], test_size=0.3, random_state=123)\n",
    "\n",
    "print(f'Train dimensions: {X_train.shape, y_train.shape}')\n",
    "print(f'Test dimensions: {X_test.shape, y_test.shape}')\n",
    "\n",
    "\n",
    "print(y_train.value_counts()) #34k comments to train, 14k to test\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dbc8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data\n",
    "def preprocess_text(text):\n",
    "    # Tokenise words while ignoring punctuation\n",
    "    tokeniser = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "    # Lowercase and lemmatise \n",
    "    porter = PorterStemmer()\n",
    "    lemmas = [porter.stem(token.lower()) for token in tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec2a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> I believe that moving forward, we should push for much higher taxes on those earning/worth more than 1 billion USD.\r\n",
      "\r\n",
      "How do you propose that this is done? You should note that billionaires do not earn billions, they own things that are worth billions.\n",
      "['i', 'believ', 'that', 'move', 'forward', 'we', 'should', 'push', 'for', 'much', 'higher', 'tax', 'on', 'those', 'earn', 'worth', 'more', 'than', '1', 'billion', 'usd', 'how', 'do', 'you', 'propos', 'that', 'thi', 'is', 'done', 'you', 'should', 'note', 'that', 'billionair', 'do', 'not', 'earn', 'billion', 'they', 'own', 'thing', 'that', 'are', 'worth', 'billion']\n"
     ]
    }
   ],
   "source": [
    "#Testing the preprocessing\n",
    "print(sample.iloc[2]['body'])\n",
    "var = preprocess_text(sample.iloc[2]['body'])\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f53123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67694, 41723)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of TfidfVectorizer\n",
    "vectoriser = TfidfVectorizer(analyzer=preprocess_text)\n",
    "# Fit to the data and transform to feature matrix\n",
    "X_train_tfidf = vectoriser.fit_transform(X_train.values.astype('U'))\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2be54147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89511781 0.8940099  0.8985154  0.89452692 0.89097356]\n",
      "Accuracy: 0.89 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "#Modeling!!!\n",
    "sgd_clf = LogisticRegression(solver='lbfgs', max_iter=120000)#SGDClassifier(random_state=123)\n",
    "sgf_clf_scores = cross_val_score(sgd_clf, X_train_tfidf, y_train, cv=5)\n",
    "print(sgf_clf_scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (sgf_clf_scores.mean(), sgf_clf_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2be5f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectoriser',\n",
       "                 TfidfVectorizer(analyzer=<function preprocess_text at 0x000002A8F49E0D30>)),\n",
       "                ('classifier', LogisticRegression(max_iter=120000))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('vectoriser', vectoriser),\n",
    "                 ('classifier', sgd_clf)])\n",
    "pipe.fit(X_train.values.astype('U'), y_train.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "676c2a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n",
      "[[12904  1229]\n",
      " [ 1683 13197]]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = pipe.predict(X_test.values.astype('U'))\n",
    "print(\"Accuracy: %0.2f\" % (accuracy_score(y_test.values.astype('U'), y_test_pred)))\n",
    "print(confusion_matrix(y_test.values.astype('U'), y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d7072df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Making a test set of unlabled data\n",
    "#rows of csv\n",
    "comment_bodies = []\n",
    "\n",
    "#colls of csv\n",
    "details = ['body']\n",
    "\n",
    "myTests = [\"this is a statement.\", \"you should not go to college\", \"those speakers are loud\",\n",
    "           \"Since i like vanilla, you should get vanilla ice cream instead of chocolate\", \"kpop is short for korean pop\",\n",
    "           \"please dont listen to kpop because it is bad\", \"you should come to my party this weekend\", \n",
    "           \"Coke was invented in atlanta georiga by John Stith Pemberton\", \n",
    "           \"If you are from atlanta, it is important to drink coke and not pepsi\",\n",
    "           \"the bears are a really good nfl team\", \"if youre coming to my party, you better be wearing some bears merch\",\n",
    "          \"please dont listen to kpop because it is bad. please dont listen to kpop because it is bad. please dont listen to kpop because it is bad. please dont listen to kpop because it is bad. please dont listen to kpop because it is bad. please dont listen to kpop because it is bad\"]\n",
    "\n",
    "#for writing to the csv file\n",
    "with open('testSet.csv', 'a') as f:\n",
    "    write = csv.writer(f)\n",
    "    try:\n",
    "        write.writerow(details)\n",
    "    except:\n",
    "        print(\"woop woop\") #used to catch an error (i think caused by untypable characters (emojis))\n",
    "    for test in myTests:\n",
    "        curr_test = test\n",
    "        arr = [curr_test]\n",
    "        try:\n",
    "            write.writerow(arr) #appends comment to csv file\n",
    "        except:\n",
    "            print(\"woop woop\") #same as other one\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df56497e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                  this is a statement.\n",
      "1                          you should not go to college\n",
      "2                               those speakers are loud\n",
      "3     Since i like vanilla, you should get vanilla i...\n",
      "4                          kpop is short for korean pop\n",
      "                            ...                        \n",
      "7     Coke was invented in atlanta georiga by John S...\n",
      "8     If you are from atlanta, it is important to dr...\n",
      "9                  the bears are a really good nfl team\n",
      "10    if youre coming to my party, you better be wea...\n",
      "11    please dont listen to kpop because it is bad. ...\n",
      "Name: body, Length: 12, dtype: object\n"
     ]
    }
   ],
   "source": [
    "man_test = pd.read_csv('testSet.csv', encoding = \"latin1\", engine='python', usecols=['body'])\n",
    "#sample = pd.DataFrame(temp, columns=['body', 'containsPersuasion'])\n",
    "man_test.head()\n",
    "man_X_test = man_test['body']\n",
    "print(man_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eecb9f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a statement.: 0\n",
      "you should not go to college: 1\n",
      "those speakers are loud: 0\n",
      "Since i like vanilla, you should get vanilla ice cream instead of chocolate: 0\n",
      "kpop is short for korean pop: 0\n",
      "you should come to my party this weekend: 0\n",
      "Coke was invented in atlanta georiga by John Stith Pemberton: 0\n",
      "If you are from atlanta, it is important to drink coke and not pepsi: 1\n",
      "the bears are a really good nfl team: 0\n",
      "if youre coming to my party, you better be wearing some bears merch: 0\n"
     ]
    }
   ],
   "source": [
    "predictions = pipe.predict(man_X_test.values.astype('U'))\n",
    "count = 0\n",
    "for i in predictions:\n",
    "    print(man_X_test.iloc[count] + \": \" + predictions[count])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940309a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa369a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}